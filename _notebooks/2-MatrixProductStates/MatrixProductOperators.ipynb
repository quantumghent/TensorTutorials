{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62317d40",
   "metadata": {},
   "source": [
    "# Matrix Product Operators\n",
    "\n",
    "If Matrix Product States are a tensor network way of representing quantum states in one\n",
    "dimensions, we can similarly use tensor networks to represent the operators that act on\n",
    "these states. Matrix Product Operators (MPOs) form a structured and convenient description\n",
    "of such operators, that can capture most (if not all) relevant operators. Additionally, they\n",
    "also form a natural way of representing the transfer matrix of a 2D statistical mechanical\n",
    "system, and can even be used to study higher dimensional systems by mapping them to quasi-1D\n",
    "systems.\n",
    "\n",
    "In general, an MPO is a chain of tensors, where each tensor has two physical indices and two\n",
    "virtual indices:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc48153",
   "metadata": {},
   "source": [
    "## Statistical Mechanics in 2D\n",
    "\n",
    "Before discussing one-dimensional transfer matrices, let us first consider how partition\n",
    "functions of two-dimensional classical many-body systems can be naturally represented as a\n",
    "tensor network. To this end, consider the partition function of the\n",
    "[classical Ising model](https://en.wikipedia.org/wiki/Ising_model),\n",
    "\n",
    "$$\n",
    "\\mathcal Z = \\sum_{\\{s_i\\}} \\text{e}^{-\\beta H(\\{s_i\\})},\n",
    "$$\n",
    "\n",
    "where $ s_i $ denotes a configuration of spins, and $ H(\\{s_i\\}) $ is the corresponding\n",
    "energy, as determined by the Hamiltonian:\n",
    "\n",
    "$$\n",
    "H(\\{s_i\\}) = -J \\sum_{\\langle i,j \\rangle} s_i s_j\n",
    "$$\n",
    "\n",
    "where the first sum is over nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c402ae24",
   "metadata": {},
   "source": [
    "### Partition Functions as Tensor Networks\n",
    "\n",
    "As the expression for the partition function is an exponential of a sum, we can also write\n",
    "it as a product of exponentials, which can be reduced to the following network:\n",
    "\n",
    "![https://quantumghent.github.io/TensorTutorials/_static/figures/mpo/partition_function_1.svg](https://quantumghent.github.io/TensorTutorials/_static/figures/mpo/partition_function_1.svg)\n",
    "\n",
    "Here, the black dots at the vertices represent Kronecker $ \\delta $-tensors,\n",
    "\n",
    "![https://quantumghent.github.io/TensorTutorials/_static/figures/mpo/kronecker.svg](https://quantumghent.github.io/TensorTutorials/_static/figures/mpo/kronecker.svg)\n",
    "\n",
    "and the matrices $ t $ encode the Boltzmann weights associated to each nearest-neighbor interaction,\n",
    "\n",
    "![https://quantumghent.github.io/TensorTutorials/_static/figures/mpo/boltzmann.svg](https://quantumghent.github.io/TensorTutorials/_static/figures/mpo/boltzmann.svg)\n",
    "\n",
    "It is then simple, albeit somewhat involved to check that contracting this network gives\n",
    "rise to the partition function, where the sum over all configurations is converted into the\n",
    "summations in the contractions of the network. Finally, it is more common to absorb the edge\n",
    "tensors into the vertex tensors by explicitly contracting them, such that the remaining\n",
    "network consists of tensors at the vertices only:\n",
    "\n",
    "![https://quantumghent.github.io/TensorTutorials/_static/figures/mpo/partition_function.svg](https://quantumghent.github.io/TensorTutorials/_static/figures/mpo/partition_function.svg)\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Because there are two edges per vertex, an intuitive way of absorbing the edge tensors is to\n",
    "absorb for example the left and bottom edge tensors into the vertex tensor. However, this\n",
    "leads to a slightly asymmetric form, and more commonly the square root $ q $ of the Boltzmann\n",
    "matrices is taken, such that each vertex tensor absorbs such a factor from each of the\n",
    "edges, resulting in a rotation-invariant form.\n",
    "\n",
    "![https://quantumghent.github.io/TensorTutorials/_static/figures/mpo/boltzmann_mpo.svg](https://quantumghent.github.io/TensorTutorials/_static/figures/mpo/boltzmann_mpo.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4ac32",
   "metadata": {},
   "source": [
    "### Transfer Matrices\n",
    "\n",
    "In order to then evaluate the partition function, we can use the\n",
    "[Transfer-matrix method](https://en.wikipedia.org/wiki/Transfer-matrix_method), which is a\n",
    "technique that splits the two-dimensional network into rows (or columns) of so-called\n",
    "transfer matrices, which are already represented as MPOs. In fact, this method has even led\n",
    "to the famous exact solution of the two-dimensional Ising model by Onsager.\n",
    "[[Onsager, 1944](https://quantumghent.github.io/TensorTutorials/../References.html#id20)].\n",
    "\n",
    "In the context of tensor networks, this technique is even useful beyond exactly solvable\n",
    "cases, as efficient algorithms exist to determine the product of an MPO with an MPS in an\n",
    "approximate manner. This allows us to efficiently split the computation of the partition\n",
    "function in a sequence of one-dimensional contractions, thus reducing the complexity of the\n",
    "problem by solving it layer by layer. For example, one can resort to *boundary MPS\n",
    "techniques* [[Zauner-Stauber *et al.*, 2018](https://quantumghent.github.io/TensorTutorials/../References.html#id14)].\n",
    "\n",
    "![https://quantumghent.github.io/TensorTutorials/_static/figures/alg/boundary_mps.svg](https://quantumghent.github.io/TensorTutorials/_static/figures/alg/boundary_mps.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7206fc",
   "metadata": {},
   "source": [
    "### Thermodynamic Limit\n",
    "\n",
    "Importantly, this technique is not limited to finite systems, and in fact allows for the\n",
    "computation of the partition function of systems directly in the thermodynamic limit,\n",
    "alleviating the need to consider finite-size effects and extrapolation techniques. The key\n",
    "insight that allows for this is that the partition function may be written as\n",
    "\n",
    "$$\n",
    "\\mathcal Z = \\lim_{N \\to \\infty} \\mathrm{Tr} \\left( T^N \\right)\n",
    "$$\n",
    "\n",
    "where $ T $ is the row-to-row transfer matrix, and $ N $ is the number of rows (or columns) in\n",
    "the network. If we then consider the spectral decomposition of the transfer matrix, we can\n",
    "easily show that as the number of rows goes to infinity, the largest eigenvalue of the\n",
    "transfer matrix dominates, and the partition function is given by\n",
    "\n",
    "$$\n",
    "\\mathcal Z = \\lim_{N \\to \\infty} \\lambda_{\\mathrm{max}}^N\n",
    "$$\n",
    "\n",
    "where $ \\lambda_{\\mathrm{max}} $ is the largest eigenvalue of the transfer matrix. In other\n",
    "words, the partition function can be computed if it is possible to find the largest\n",
    "eigenvalue of the transfer matrix, for which efficient algorithms exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2543847c",
   "metadata": {},
   "source": [
    "### Expectation Values\n",
    "\n",
    "In order to compute relevant quantities for such systems, we can verify that the expectation\n",
    "value of an operator $ O $ is given by the weighing the value of that operator for a given\n",
    "microstate, with the probability of that microstate:\n",
    "\n",
    "$$\n",
    "\\langle O \\rangle = \\frac{1}{\\mathcal Z} \\sum_{\\{s_i\\}} O(\\{s_i\\})\\text{e}^{-\\beta\n",
    "H(\\{s_i\\})}\n",
    "$$\n",
    "\n",
    "For a local operator $ O_i $, this can again be written as a tensor network, where a single\n",
    "vertex tensor is exchanged for $ M\\prime_{ijkl} = O_i \\delta_{ijkl} $, and then absorbing the\n",
    "remaining edge tensors:\n",
    "\n",
    "Using this network, the expectation value can be computed by first contracting the top and\n",
    "bottom part, replacing them by their fixed-point MPS representations, and then contracting\n",
    "the remaining MPS-MPO-MPS sandwich. This is achieved by similarly contracting the left and\n",
    "right part, replacing them by their fixed-point tensors, which are commonly called the\n",
    "*environments* $ G_L $ and $ G_R $, respectively. The final resulting network is then just a\n",
    "local network, which can be contracted efficiently.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">This process of sequentally reducing the dimensionality of the network can even be further\n",
    "extended, where 3D systems can be studied by first determining a 2D boundary PEPS, for which\n",
    "a 1D boundary MPS can be determined, which admits 0D boundary tensors. This kind of\n",
    "algorithms are commonly referred to as *boundary methods*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346dd948",
   "metadata": {},
   "source": [
    "## Quantum Mechanics in 1+1D\n",
    "\n",
    "For quantum systems in one spatial dimension, the construction of MPOs boils down to the\n",
    "ability to write a sum of local operators in MPO-form. The resulting operator has a very\n",
    "specific structure, and is often referred to as a *Jordan block MPO*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeb8a08",
   "metadata": {},
   "source": [
    "### Jordan Block MPOs\n",
    "\n",
    "For example, if we consider the\n",
    "[Transverse-field Ising model](https://en.wikipedia.org/wiki/Transverse-field_Ising_model),\n",
    "\n",
    "$$\n",
    "H = -J \\sum X_j X_{j+1} - h \\sum Z_j\n",
    "$$\n",
    "\n",
    "it can be represented as an MPO through the (operator-valued) matrix,\n",
    "\n",
    "$$\n",
    "W = \\begin{pmatrix}\n",
    "1 & X & -hZ \\\\ \n",
    "0 & 0 & -JX \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "along with the boundary vectors,\n",
    "\n",
    "$$\n",
    "v_L = \\begin{pmatrix}\n",
    "1 & 0 & 0\n",
    "\\end{pmatrix}\n",
    ", \\qquad \n",
    "v_R = \\begin{pmatrix}\n",
    "0 \\\\ 0 \\\\ 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The Hamiltonian on $ N $ sites is then given by the contraction\n",
    "\n",
    "$$\n",
    "H = V_L W^{\\otimes N} V_R\n",
    "$$\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">While the above example can be constructed from building blocks that are strictly local\n",
    "operators, this is not always the case, especially when symmetries are involved. In those\n",
    "cases, the elements of the matrix $ W $ have additional virtual legs that are contracted\n",
    "between different sites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c59b7a4",
   "metadata": {},
   "source": [
    "### Finite-State Machines\n",
    "\n",
    "An intuitive approach to construct such MPOs is to consider the sum of local\n",
    "terms by virtue of a\n",
    "[finite-state machine](https://en.wikipedia.org/wiki/Finite-state_machine). This is a\n",
    "mathematical model of computation that consists of a finite set of states, and a set of\n",
    "transitions between those states. In the context of MPOs, this is realised by associating\n",
    "each *virtual level* with a state, and each transition then corresponds to applying a local\n",
    "operator. In that regard, the MPO is then a representation of the state of the finite-state\n",
    "machine, and the matrix $ W $ is the transition matrix of the machine.\n",
    "\n",
    "In general, the matrix $ W $ can then be thought of as a block matrix with entries\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & C & D \\\\\n",
    "0 & A & B \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "which corresponds to the finite-state diagram:\n",
    "\n",
    "It can then be shown that this MPO generates all single-site local operators $ D $, two-site\n",
    "operators $ CB $, three-site operators $ CAB $, and so on. In other words, the MPO is a\n",
    "representation of the sum of all local operators, and by carefully extending the structure\n",
    "of the blocks $ A $, $ B $, $ C $, and $ D $, it is possible to construct MPOs that represent sums\n",
    "of generic local terms, and even approximate long-range interactions by a sum of\n",
    "exponentials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45e3e7",
   "metadata": {},
   "source": [
    "### Expectation Values\n",
    "\n",
    "In order to compute expectation values of such MPOs, we can use the same technique as\n",
    "before, and sandwich the MPO between two MPSs.\n",
    "\n",
    "However, care must be taken when the goal is to determine a local expectation value density,\n",
    "as this is not necessarily well-defined. In fact, the MPO-MPS sandwich will always lead to\n",
    "the total energy, and in order to consistently define local contributions, a choice must be\n",
    "made how to *distribute* this among the sites. For example, even in the case of two-site\n",
    "local operators, it is unclear if this local expectation value should be accredited to the\n",
    "left, right, or both sites. In the implementation of MPSKit, the chosen convention is to\n",
    "distribute the expectation value evenly among its starting and ending point, in order to not\n",
    "overcount contributions of long-range interactions.\n",
    "\n",
    "Thus, the computation of local expectation values is done by first contracting the left and\n",
    "right ends of the network, and denoting the resulting tensors as $ G_L $ and $ G_R $,\n",
    "respectively. These tensors are typically referred to as the *environments* of the network,\n",
    "as they depict the part of the network that is not under consideration, and that is thus\n",
    "considered as constant.\n",
    "\n",
    "Then, the local expectation value is given by the mean of the contraction with the first row\n",
    "of $ M $, which is equivalent to all terms starting at that site, and the last column of $ M $,\n",
    "which is equivalent to all terms ending at that site.\n",
    "\n",
    "Again, it is instructive to write this out explicitly for some small examples to gain some\n",
    "intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3e8c65",
   "metadata": {},
   "source": [
    "### Jordan MPOs in the Thermodynamic Limit\n",
    "\n",
    "In the thermodynamic limit, the same MPO construction can be used to represent the infinite\n",
    "sum of local terms. However, special care must be taken when considering expectation values,\n",
    "as now only local expectation values are well-defined, and the total energy diverges with\n",
    "the system size.\n",
    "\n",
    "This is achieved by considering a regularization of the environment tensors, such that the\n",
    "divergent parts are already removed. This construction can be found in more detail in\n",
    "[]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147a331",
   "metadata": {},
   "source": [
    "### Quasi-1D Systems\n",
    "\n",
    "Finally, it is worth noting that the MPO construction can also be used to study\n",
    "two-dimensional systems, by mapping them to quasi-one-dimensional systems. This is typically\n",
    "achieved by imposing periodic boundary conditions in one of the spatial directions, and then\n",
    "*snaking* an MPS through the resulting lattice. In effect, this leads to a one-dimensional\n",
    "model with longer-range interactions, which can then be studied using the standard MPS\n",
    "techniques. However, the\n",
    "[no free lunch theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem) applies here as\n",
    "well, and the resulting model will typically require a bond dimension that grows\n",
    "exponentially with the periodic system size, in order to achieve the area law of\n",
    "entanglement in two-dimensional systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d8e43",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, Matrix Product Operators are a powerful tool to represent quantum operators\n",
    "as well as transfer matrices. They allow for efficient and versatile expressions of\n",
    "expectation values, and form the building block for many tensor network algorithms, both in\n",
    "(1+1) or (2+0) dimensions, as well as in higher dimensions."
   ]
  }
 ],
 "metadata": {
  "date": 1695224380.7965722,
  "filename": "MatrixProductOperators.md",
  "kernelspec": {
   "display_name": "Julia",
   "language": "julia",
   "name": "julia-1.9"
  },
  "title": "Matrix Product Operators"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}