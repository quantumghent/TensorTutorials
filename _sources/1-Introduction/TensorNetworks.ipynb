{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb27102d",
   "metadata": {},
   "source": [
    "# Tensor Network Theory\n",
    "\n",
    "```{contents} Contents\n",
    ":depth: 2\n",
    "```\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lecture we will introduce the basic concepts of tensor network theory. We will start\n",
    "with a brief overview of the history of tensor networks and their relevance to modern\n",
    "physics. We will then introduce the graphical notation that is often used to simplify\n",
    "expressions, and discuss the relevant operations and decompositions along with their\n",
    "computation complexity and their relevance to quantum many-body physics.\n",
    "\n",
    "This discussion is largely based on {cite}`bridgeman2017handwaving`.\n",
    "\n",
    "This lecture also serves as a brief introduction to\n",
    "[TensorOperations.jl](https://github.com/Jutho/TensorOperations.jl), and showcases some more\n",
    "features of [TensorKit.jl](https://github.com/Jutho/TensorKit.jl) as well. Note that\n",
    "TensorKit already re-exports the `@tensor` macro from TensorOperations, so it is not\n",
    "necessary to import it separately if TensorKit is already loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3363204b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling TensorKit [07d1fe3e-3e46-537d-9eac-e9e13d0d4cec]\n"
     ]
    }
   ],
   "source": [
    "using TensorKit\n",
    "using Test # for showcase testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d8f1b9",
   "metadata": {},
   "source": [
    "### History\n",
    "\n",
    "The history of tensor networks is a fascinating journey through the evolution of profound\n",
    "theoretical ideas and evolutions, as well as the development of computational methods and\n",
    "tools. These ideas have been developed in a variety of contexts, but have been especially\n",
    "relevant to the study of quantum physics and machine learning.\n",
    "\n",
    "1. Early Foundations:\n",
    "  * The roots of tensor networks can be traced back to the early development of linear algebra and matrix notation in the 19th century, pioneered by mathematicians like Arthur Cayley and James Sylvester.\n",
    "  * The concept of tensors as multi-dimensional arrays of numbers began to emerge in the late 19th and early 20th centuries.\n",
    "2. Matrix Product States and DMRG:\n",
    "  * The birth of modern tensor network theory can be attributed to the introduction of MPS in the 1960s (?).\n",
    "  * One of the earliest, and still most widely used tensor network algorithm is DMRG. It was developed by Steven White in 1992, and provides one of the most efficient methods for simulating one-dimensional quantum many-body systems.\n",
    "3. Quantum Information Theory:\n",
    "  * In the 1980s and 1990s, the field of quantum information theory began to emerge, driven by (add names here)\n",
    "  * Concepts such as quantum entanglement and quantum information became central to the study of quantum many-body systems.\n",
    "4. Higher-Dimensional Tensor Networks:\n",
    "  * As the field progressed, tensor network methods were extended to higher-dimensional systems, leading to the emergence of more general tensor network states (TNS)..\n",
    "  * Two-dimensional tensor networks such as Projected Entangled Pair States (PEPS) and Multi-scale Entanglement Renormalization Ansatz (MERA) were introduced in the early 2000s.\n",
    "5. Tensor Networks in other disciplines:\n",
    "  * Many of the concepts and methods developed in the context of tensor networks have been applied to other disciplines, one of the most prominent being machine learning.\n",
    "  * Unsuprisingly, they also play a central role in quantum computing, where tensor network algorithms provide a natural language to explore quantum circuit simulations.\n",
    "6. Ongoing Research and Applications\n",
    "  * Tensor network theory continues to be a vibrant and evolving field with ongoing research in various directions, such as the development of efficient tensor contraction algorithms, the application of tensor networks for understanding quantum phases of matter, the development of tensor network algorithms for quantum computing, and the application of tensor networks to machine learning.\n",
    "\n",
    "## Graphical Notation and Tensor Operations\n",
    "\n",
    "One of the main advantages of tensor networks is that they admit a very intuitive graphical\n",
    "notation, which greatly simplifies the expressions involving numerous indices. This notation\n",
    "is based on the idea of representing a single tensor as a node in a graph, where the indices\n",
    "of the tensor are depicted by legs sticking out of it, one for each vector space. As an\n",
    "example, a rank-four tensor $R$ can be represented as:\n",
    "\n",
    "```{image} /_static/TensorNetworks/R-tensor.svg\n",
    ":scale: 12%\n",
    ":name: R-tensor\n",
    ":align: center\n",
    "```\n",
    "\n",
    "### Indexing\n",
    "\n",
    "In this notation, the individual components of the tensor can be recoverd by fixing the open\n",
    "legs of a diagram to some value, and the resulting diagram is then a scalar. For example,\n",
    "the component $R_{i_1,i_2,i_3,i_4}$ is given by:\n",
    "\n",
    "```{image} /_static/TensorNetworks/indexing.svg\n",
    ":scale: 12%\n",
    ":name: indexing\n",
    ":align: center\n",
    "```\n",
    "\n",
    "### Grouping and Splitting of Indices\n",
    "\n",
    "Because of the isomorphism {eq}`eq:tensor_isomorphism`, the legs of the tensor can be freely\n",
    "moved around, as long as their order is preserved. In some contexts the shape of\n",
    "the node and the direction of the tensor can imply certain properties, such as making an\n",
    "explicit distinction between the isomorphic representations, but in what follows we will not\n",
    "make this distinction.\n",
    "\n",
    "Furthermore, this naturally gives a notion of grouping and splitting of indices, which is\n",
    "just a reinterpretation of a set of neighbouring vector spaces as a single vector space, and\n",
    "the inverse operation. For example, the following diagrams are equivalent:\n",
    "\n",
    "```{image} /_static/TensorNetworks/grouping.svg\n",
    ":scale: 12%\n",
    ":name: grouping\n",
    ":align: center\n",
    "```\n",
    "\n",
    "Owing to the freedom in choice of basis, the precise details of grouping and splitting are\n",
    "not unique. One specific choice of convention is the tensor product basis, which is\n",
    "precisely the one we have used in the discussion of multi-linear algebra. More concretely,\n",
    "one choice that is often used is the _Kronecker product_, which in the setting of\n",
    "column-major ordering is given explicitly by grouping indices as follows:\n",
    "\n",
    "```{math}\n",
    ":label: eq:kronecker_product\n",
    "I := i_1 + d_1 * (i_2 - 1) + d_1 * d_2 * (i_3 - 1) + d_1 * d_2 * d_3 * (i_4 - 1) + \\cdots\n",
    "```\n",
    "\n",
    "Here $d_i$ is the dimension of the corresponding vector space, and $I$ is the resulting\n",
    "linear index. Note again that so long as the chosen convention is consistent, the precise\n",
    "method of grouping and splitting is immaterial.\n",
    "\n",
    "This can be conveniently illustrated by the `reshape` function in Julia, which performs\n",
    "exactly this operation. For simple arrays, this operation does nothing but change the size\n",
    "property of the data structure, as the underlying data necessarily needs to be stored in a\n",
    "linear order in memory, as computer adresses are linear. Because of this, in tensor\n",
    "networks, these operations are typically left implicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a52c74f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×4×2 reshape(::UnitRange{Int64}, 2, 4, 2) with eltype Int64:\n",
       "[:, :, 1] =\n",
       " 1  3  5  7\n",
       " 2  4  6  8\n",
       "\n",
       "[:, :, 2] =\n",
       "  9  11  13  15\n",
       " 10  12  14  16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = reshape(1:(2^4), (2, 2, 2, 2))\n",
    "B = reshape(A, (4, 2, 2))\n",
    "C = reshape(A, (2, 4, 2))\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07bb43e",
   "metadata": {},
   "source": [
    "### Outer Products\n",
    "\n",
    "Of course, in order to really consider a tensor *network*, it is necessary to consider\n",
    "diagrams that consist of multiple tensors, or in other words of multiple nodes. The simplest\n",
    "such diagram represents the _outer product_ of two tensors. This is represented by two\n",
    "tensors being placed next to each other. The value of the resulting network is simply the\n",
    "product of the constituents. For example, the outer product of a rank three tensor $A$ and a\n",
    "rank two tensor $B$ is given by:\n",
    "\n",
    "```{image} /_static/TensorNetworks/outer-product.svg\n",
    ":scale: 12%\n",
    ":name: outer-product\n",
    ":align: center\n",
    "```\n",
    "\n",
    "### Traces\n",
    "\n",
    "More complicated diagrams can be constructed by joining some of the legs of the constituent\n",
    "tensors. In a matter similar to the conventional Einstein notation, this implies a summation\n",
    "over the corresponding indices.\n",
    "\n",
    "If two legs from a single tensor are joined, this signifies a (partial) _trace_ of a tensor\n",
    "over these indices. For example, the trace of a rank three tensor $A$ over two of its\n",
    "indices is given by:\n",
    "\n",
    "```{image} /_static/TensorNetworks/trace.svg\n",
    ":scale: 12%\n",
    ":name: trace\n",
    ":align: center\n",
    "```\n",
    "\n",
    "In this notation, the cyclic property of the trace follows by sliding one of the matrices\n",
    "around the loop of the diagram. As this only changes the placement of the tensors in the\n",
    "network, and not the value, the graphic proof of $\\text{Tr}(AB) = \\text{Tr}(BA)$ is found.\n",
    "\n",
    "```{image} /_static/TensorNetworks/trace-cyclic.svg\n",
    ":scale: 12%\n",
    ":name: trace-cyclic\n",
    ":align: center\n",
    "```\n",
    "\n",
    "### Contractions\n",
    "\n",
    "The most common tensor operation used is _contraction_, which is the joining of legs from\n",
    "different tensors. This can equivalently be thought of as a tensor product followed by a\n",
    "trace. For example, the contraction between two pairs of indices of two rank-three tensors\n",
    "is drawn as:\n",
    "\n",
    "```{image} /_static/TensorNetworks/contraction.svg\n",
    ":scale: 12%\n",
    ":name: contraction\n",
    ":align: center\n",
    "```\n",
    "\n",
    "Famililiar examples of contraction are vector inner products, matrix-vector multiplication,\n",
    "matrix-matrix multiplication, and matrix traces.\n",
    "\n",
    "```{image} /_static/TensorNetworks/vecvec.svg\n",
    ":scale: 12%\n",
    ":name: vecvec\n",
    ":align: center\n",
    "```\n",
    "\n",
    "```{image} /_static/TensorNetworks/matvec.svg\n",
    ":scale: 12%\n",
    ":name: matvec\n",
    ":align: center\n",
    "```\n",
    "\n",
    "```{image} /_static/TensorNetworks/matmat.svg\n",
    ":scale: 12%\n",
    ":name: matmat\n",
    ":align: center\n",
    "```\n",
    "\n",
    "```{image} /_static/TensorNetworks/tr.svg\n",
    ":scale: 12%\n",
    ":name: tr\n",
    ":align: center\n",
    "```\n",
    "\n",
    "\n",
    "## Network Contractions\n",
    "\n",
    "Combining the operations defined above, it is possible to construct arbitrarily complicated\n",
    "_tensor networks_, which can then be evaluated by a sequence of pair-wise operations. The\n",
    "result then reduces to a tensor which has a rank equal to the number of open legs in the\n",
    "network. For example, the following diagram represents a generic tensor network:\n",
    "\n",
    "```{image} /_static/TensorNetworks/network.svg\n",
    ":scale: 12%\n",
    ":name: network\n",
    ":align: center\n",
    "```\n",
    "\n",
    "### Notation\n",
    "\n",
    "In order to evaluate such networks, it is necessary to define a notational convention for\n",
    "specifying a network with text. One of the most common conventions is that of\n",
    "[Einstein notation](https://en.wikipedia.org/wiki/Einstein_notation), where each index of a\n",
    "tensor is assigned a label, and repeated labels are implicitly summed over. For example, the outer product, trace, and inner product can respectively be obtained as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad5a4b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2, 2, 2, 2), (2,), (2, 2, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(2, 2, 2)\n",
    "B = rand(2, 2)\n",
    "@tensor C[i, j, k, l, m] := A[i, j, k] * B[l, m]\n",
    "@tensor D[i] := A[i, j, j]\n",
    "@tensor E[i, j, l] := A[i, j, k] * B[l, k]\n",
    "size(C), size(D), size(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e602d89f",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The `@tensor` macro can be used to either create new tensors, using the `:=` assignment, or\n",
    "to copy data into existing tensors using `=`. In the latter case, the tensor must already\n",
    "exist and have the right dimensions, but less additional memory is allocated.\n",
    "```\n",
    "\n",
    "This notation is very useful indeed, but quickly becomes unwieldy when one wishes to specify\n",
    "in what order the pairwise operations should be carried out. Thus, in the same spirit but\n",
    "with a minor modification, the [NCON](https://arxiv.org/abs/1402.0939) notation was\n",
    "introduced. In this notation, the indices of a tensor are assigned integers, and pairwise\n",
    "operations happen in increasing order. Similarly, negative integers are assigned to open\n",
    "legs, which determine their resulting position. For example, the diagram from\n",
    "{image}`network` can be written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50e7132b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Float64}:\n",
       " 4.04819  4.94825\n",
       " 4.01989  4.84833"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = rand(2, 2, 2, 2)\n",
    "C = rand(2, 2, 2, 2, 2)\n",
    "D = rand(2, 2, 2)\n",
    "E = rand(2, 2)\n",
    "F = rand(2, 2)\n",
    "@tensor begin\n",
    "    A[-1, -2] := B[-1, 1, 2, 3] * C[3, 5, 6, 7, -2] * D[2, 4, 5] * E[1, 4] * F[6, 7]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98046f2b",
   "metadata": {},
   "source": [
    "### Contraction Order and Complexity\n",
    "\n",
    "While tensor networks are defined in such a way that their values are independent of the\n",
    "order of pairwise operations, the computational complexity of evaluating a network can vary\n",
    "wildly based on the chosen order. Even for simple matrix-matrix-vector multiplication, the\n",
    "problem can easily be illustrated by considering the following two equivalent operations:\n",
    "\n",
    "```{math}\n",
    "w = A * (B * v) = (A * B) * v\n",
    "```\n",
    "\n",
    "If both $A$ and $B$ are square matrices of size $N \\times N$, and $v$ and $w$ are vectors of\n",
    "length $N$, the first operation requires $2N^2$ floating point operations (flops), while the\n",
    "second requires $N^3 + N^2$ flops. This is a substantial difference, and it is clear that\n",
    "the first operation is to be preferred.\n",
    "\n",
    "More generally, the amount of flops required for contracting a pair of tensors can be\n",
    "determined by considering the fact that the amount of elements to compute is equal to the\n",
    "product of the dimensions of the open indices, and the amount of flops required to compute\n",
    "each element is equal to the product of the dimensions of the contracted indices. Due to\n",
    "this fact, it is typically the most efficient to _minimize the surface area of contraction_,\n",
    "which boils down to the heuristic of minimizing the amount of legs that are cut, also known\n",
    "as _bubbling_.\n",
    "\n",
    "Many networks admit both efficient and inefficient contraction orders, and often it is\n",
    "infeasible to compute the optimal order. Take for example a ladder-shaped network, which is\n",
    "of particular relevance in the context of Matrix Product States, we can highlight a few\n",
    "possible contraction orders, for which we leave it as an exercise to determine the\n",
    "computational complexity:\n",
    "\n",
    "```{image} /_static/TensorNetworks/ladder1.svg\n",
    ":scale: 12%\n",
    ":name: ladder1\n",
    ":align: center\n",
    "```\n",
    "\n",
    "```{image} /_static/TensorNetworks/ladder2.svg\n",
    ":scale: 12%\n",
    ":name: ladder2\n",
    ":align: center\n",
    "```\n",
    "\n",
    "Determining the optimal order however is a problem that is known to be NP-hard, and thus no\n",
    "algorithm exists that can efficiently compute optimal orders for larger networks.\n",
    "Nevertheless, efficient implementations allows finding optimal orders for networks of up to\n",
    "30-40 tensors {cite}`pfeifer2014faster`, but other methods exist that can be used to\n",
    "determine good (not necessarily optimal) contraction orders.\n",
    "\n",
    "TensorOperations comes with some built-in tools for facilitating this process, and in\n",
    "particular the `opt` keyword can be used to enable the use of the algorithm from\n",
    "{cite}`pfeifer2014faster`. Because this uses the Julia macro system, this can be done at\n",
    "compilation time, and in other words only needs to be computed once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1360cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Float64}:\n",
       " 4.04819  4.94825\n",
       " 4.01989  4.84833"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tensor opt=true begin\n",
    "    A[i, j] := B[i, α, β, γ] * C[γ, ϵ, ζ, η, j] * D[β, δ, ϵ] * E[α, δ] * F[ζ, η]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3fdf1e",
   "metadata": {},
   "source": [
    "## Tensor Factorizations\n",
    "\n",
    "Linear maps admit various kinds of factorizations, which are instrumental in a variety of\n",
    "applications. They can be used to generate orthogonal bases, to find low-rank\n",
    "approximations, or to find eigenvalues and vectors. In the context of tensors, the\n",
    "established theory for factorizations of matrices can be generalized by interpreting tensors\n",
    "as linear maps, and then applying the same factorization to the corresponding matrix\n",
    "partition of the constituent vector spaces in a codomain and domain, after which everything\n",
    "carries over. Thus, the only additional information that is required is the specification of\n",
    "this partition. In this section we will discuss the most common factorizations of tensors,\n",
    "but the reasoning can be generalized to any factorization of linear maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dae282c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ℂ^2 ⊗ ℂ^3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S1 = ℂ^2 ⊗ ℂ^2 ⊗ ℂ^2\n",
    "S2 = ℂ^2 ⊗ ℂ^3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598bf7e1",
   "metadata": {},
   "source": [
    "### Eigenvalue Decomposition\n",
    "\n",
    "The [Eigen decomposition of a matrix](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix)\n",
    "$A$ is a factorization of the form:\n",
    "\n",
    "```{math}\n",
    "A = V \\Lambda V^{-1}\n",
    "```\n",
    "\n",
    "where $V$ is a matrix of eigenvectors, and $\\Lambda$ is a diagonal matrix of eigenvalues. In\n",
    "particular, the set of eigenvectors form a basis for all possible products $Ax$, which is\n",
    "the same as the image of the corresponding matrix transformation. For normal matrices, these\n",
    "eigenvectors can be made orthogonal and the resulting decomposition is also called the\n",
    "_spectral decomposition_.\n",
    "\n",
    "The eigenvalue decomposition mostly finds it use in the context of linear equations of the\n",
    "form:\n",
    "\n",
    "```{math}\n",
    "Av = \\lambda v\n",
    "```\n",
    "\n",
    "where $v$ is an eigenvector of $A$ with eigenvalue $\\lambda$.\n",
    "\n",
    "For tensors, the eigenvalue decomposition is defined similarly, and the equivalent equation\n",
    "is diagrammatically represented as:\n",
    "\n",
    "```{image} /_static/TensorNetworks/eig.svg\n",
    ":scale: 12%\n",
    ":name: eig\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a089e9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = TensorMap(randn, ComplexF64, S1, S1) # codomain and domain equal for eigendecomposition\n",
    "D, V = eig(A)\n",
    "@test A * V ≈ V * D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a287d1",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "\n",
    "The\n",
    "[Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "(SVD) can be seen as a generalization of the eigendecomposition of a square normal matrix to\n",
    "any rectangular matrix $A$. Specifically, it is a factorization of the form\n",
    "$A = U \\Sigma V^\\dagger$ where $U$ and $V$ are isometric matrices\n",
    "($U^\\dagger U = V^\\dagger V = \\mathbb{1}$), and $\\Sigma$ is a diagonal matrix of singular\n",
    "values. The SVD is typically used to find low-rank approximations for matrices, and it was\n",
    "shown {cite}`eckart1936approximation` that the best rank-$k$ approximation is given by the\n",
    "SVD, where $\\Sigma$ is truncated to the first (largest) $k$ singular values.\n",
    "\n",
    "Again, a tensorial version is defined by first grouping indices to form a matrix, and then\n",
    "applying the SVD to that matrix.\n",
    "\n",
    "```{image} /_static/TensorNetworks/svd.svg\n",
    ":scale: 12%\n",
    ":name: svd\n",
    ":align: center\n",
    "```\n",
    "\n",
    "```{image} /_static/TensorNetworks/unitary.svg\n",
    ":scale: 12%\n",
    ":name: unitary\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b576b98",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching tsvd(::TensorMap{ComplexSpace, 3, 2, Trivial, Matrix{ComplexF64}, Nothing, Nothing}, ::Tuple{Tuple{Int64, Int64}, Tuple{Int64, Int64, Int64}})\n\n\u001b[0mClosest candidates are:\n\u001b[0m  tsvd(::AbstractTensorMap; trunc, p, alg)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mTensorKit\u001b[39m \u001b[90m~/.julia/packages/TensorKit/MikvC/src/tensors/\u001b[39m\u001b[90m\u001b[4mfactorizations.jl:253\u001b[24m\u001b[39m\n\u001b[0m  tsvd(::AbstractTensorMap, \u001b[91m::Tuple{Vararg{Int64, N}} where N\u001b[39m, \u001b[91m::Tuple{Vararg{Int64, N}} where N\u001b[39m; kwargs...)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mTensorKit\u001b[39m \u001b[90m~/.julia/packages/TensorKit/MikvC/src/tensors/\u001b[39m\u001b[90m\u001b[4mfactorizations.jl:50\u001b[24m\u001b[39m\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching tsvd(::TensorMap{ComplexSpace, 3, 2, Trivial, Matrix{ComplexF64}, Nothing, Nothing}, ::Tuple{Tuple{Int64, Int64}, Tuple{Int64, Int64, Int64}})\n\n\u001b[0mClosest candidates are:\n\u001b[0m  tsvd(::AbstractTensorMap; trunc, p, alg)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mTensorKit\u001b[39m \u001b[90m~/.julia/packages/TensorKit/MikvC/src/tensors/\u001b[39m\u001b[90m\u001b[4mfactorizations.jl:253\u001b[24m\u001b[39m\n\u001b[0m  tsvd(::AbstractTensorMap, \u001b[91m::Tuple{Vararg{Int64, N}} where N\u001b[39m, \u001b[91m::Tuple{Vararg{Int64, N}} where N\u001b[39m; kwargs...)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[35mTensorKit\u001b[39m \u001b[90m~/.julia/packages/TensorKit/MikvC/src/tensors/\u001b[39m\u001b[90m\u001b[4mfactorizations.jl:50\u001b[24m\u001b[39m\n",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[8]:3"
     ]
    }
   ],
   "source": [
    "A = TensorMap(randn, ComplexF64, S1, S2)\n",
    "partition = ((1, 2), (3, 4, 5))\n",
    "U, S, V = tsvd(A, partition)\n",
    "@test permute(A, partition) ≈ U * S * V\n",
    "@test U' * U ≈ id(domain(U))\n",
    "@test V * V' ≈ id(codomain(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5915b16e",
   "metadata": {},
   "source": [
    "### Polar decomposition\n",
    "\n",
    "The [polar decomposition](https://en.wikipedia.org/wiki/Polar_decomposition) of a square\n",
    "matrix $A$ is a factorization of the form $A = UP$, where $U$ is a unitary matrix and $P$ is\n",
    "a positive semi-definite Hermitian matrix. It can be interpreted as decomposing a linear\n",
    "transformation into a rotation/reflection $U$, combined with a scaling $P$. The polar\n",
    "decomposition is unique for all matrices that are full rank.\n",
    "\n",
    "```{image} /_static/TensorNetworks/polar.svg\n",
    ":scale: 12%\n",
    ":name: svd\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2fe21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = TensorMap(randn, ComplexF64, S1, S2)\n",
    "partition = ((1, 2), (3, 4, 5))\n",
    "Q, P = leftorth(A, partition; alg=Polar())\n",
    "@test permute(A, partition) ≈ Q * P\n",
    "@test Q' * Q ≈ id(domain(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d80f9d4",
   "metadata": {},
   "source": [
    "### QR Decomposition\n",
    "\n",
    "The [QR decomposition](https://en.wikipedia.org/wiki/QR_decomposition) is a factorization of\n",
    "the form $A = QR$, where $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix.\n",
    "It is typically used to solve linear equations of the form $Ax = b$, which admits a solution\n",
    "of the form $x = R^{-1} Q^\\dagger b$. Here $R^{-1}$ is particularly easy to compute because\n",
    "of the triangular structure (for example by Gaussian elimination). Additionally, for\n",
    "overdetermined linear systems, the QR decomposition can be used to find the least-squares\n",
    "solution.\n",
    "\n",
    "```{image} /_static/TensorNetworks/qr.svg\n",
    ":scale: 12%\n",
    ":name: qr\n",
    ":align: center\n",
    "```\n",
    "\n",
    "```{image} /_static/TensorNetworks/leftOrth.svg\n",
    ":scale: 12%\n",
    ":name: leftOrth\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46308fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = TensorMap(randn, ComplexF64, S1, S2)\n",
    "partition = ((1, 2), (3, 4, 5))\n",
    "Q, R = leftorth(A, partition; alg=QR())\n",
    "@test permute(A, partition) ≈ Q * R\n",
    "@test Q' * Q ≈ id(domain(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5ead6",
   "metadata": {},
   "source": [
    "The QR decomposition is unique up to a diagonal matrix of phases, and can thus be made\n",
    "unique by requiring that the diagonal elements of $R$ are positive. This variant is often\n",
    "called QRpos. Additional variants exist that are flipped and/or transposed, such as the RQ,\n",
    "QL, and LQ decompositions.\n",
    "\n",
    "```{note}\n",
    "Often it is useful to make a distinction between factorizations that are _rank revealing_,\n",
    "and factorizations that are not. A factorization is rank revealing if the rank of the matrix\n",
    "can be determined from the factorization. For example, the SVD is rank revealing, while the\n",
    "QR decomposition is not. However, the trade-off being that the SVD decomposition is\n",
    "substantially more expensive, the QR decomposition is often preferred in practice.\n",
    "```\n",
    "\n",
    "### Nullspaces\n",
    "\n",
    "Finally, the nullspace of a matrix $A$ is the set of vectors $x$ such that $Ax = 0$. This is\n",
    "typically determined via the SVD, where the nullspace is given by the right singular vectors\n",
    "corresponding to zero singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = TensorMap(randn, ComplexF64, S1, S2)\n",
    "partition = ((1, 2), (3, 4, 5))\n",
    "N = leftnull(A, partition)\n",
    "@test norm(N' * permute(A, partition)) ≈ 0\n",
    "@test N * N' ≈ id(codomain(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd10d8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lecture we have introduced the basic concepts of tensor network theory. We have\n",
    "defined tensors and the operations that are commonly performed, as well as the graphical\n",
    "notation that is used to represent them. We have also discussed the computational complexity\n",
    "of tensor networks, and the importance of finding efficient contraction orders. Finally, we\n",
    "have discussed the most common tensor factorizations, and how they can be used."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Julia",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "source_map": [
   11,
   35,
   38,
   130,
   135,
   239,
   246,
   262,
   271,
   326,
   330,
   344,
   347,
   382,
   386,
   415,
   422,
   438,
   444,
   468,
   474,
   495,
   501
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}