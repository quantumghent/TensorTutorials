{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2377a70",
   "metadata": {},
   "source": [
    "# Tensor Network Theory\n",
    "\n",
    "```{contents} Contents\n",
    ":depth: 2\n",
    "```\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lecture we will introduce the basic concepts of tensor network theory. We will start\n",
    "with a brief overview of the history of tensor networks and their relevance to modern\n",
    "physics. We will then introduce the basic mathematical concepts of tensor networks,\n",
    "including multi-linear algebra and graphical notation. Finally, we will discuss the\n",
    "computational complexity of tensor networks and their relevance to quantum many-body\n",
    "physics.\n",
    "\n",
    "### History\n",
    "\n",
    "The history of tensor networks is a fascinating journey through the evolution of profound\n",
    "theoretical ideas and evolutions, as well as the development of computational methods and\n",
    "tools. These ideas have been developed in a variety of contexts, but have been especially\n",
    "relevant to the study of quantum physics and machine learning.\n",
    "\n",
    "1. Early Foundations:\n",
    "\n",
    "* The roots of tensor networks can be traced back to the early development of linear algebra and matrix notation in the 19th century, pioneered by mathematicians like Arthur Cayley and James Sylvester.\n",
    "* The concept of tensors as multi-dimensional arrays of numbers began to emerge in the late 19th and early 20th centuries.\n",
    "\n",
    "2. Matrix Product States and DMRG:\n",
    "\n",
    "* The birth of modern tensor network theory can be attributed to the introduction of MPS in the 1960s (?).\n",
    "* One of the earliest, and still most widely used tensor network algorithm is DMRG. It was developed by Steven White in 1992, and provides one of the most efficient methods for simulating one-dimensional quantum many-body systems.\n",
    "\n",
    "3. Quantum Information Theory:\n",
    "\n",
    "* In the 1980s and 1990s, the field of quantum information theory began to emerge, driven by (add names here)\n",
    "* Concepts such as quantum entanglement and quantum information became central to the study of quantum many-body systems.\n",
    "\n",
    "4. Higher-Dimensional Tensor Networks:\n",
    "\n",
    "* As the field progressed, tensor network methods were extended to higher-dimensional systems, leading to the emergence of more general tensor network states (TNS)..\n",
    "* Two-dimensional tensor networks such as Projected Entangled Pair States (PEPS) and Multi-scale Entanglement Renormalization Ansatz (MERA) were introduced in the early 2000s.\n",
    "\n",
    "5. Tensor Networks in other disciplines:\n",
    "\n",
    "* Many of the concepts and methods developed in the context of tensor networks have been applied to other disciplines, one of the most prominent being machine learning.\n",
    "* Unsuprisingly, they also play a central role in quantum computing, where tensor network algorithms provide a natural language to explore quantum circuit simulations.\n",
    "\n",
    "6. Ongoing Research and Applications\n",
    "\n",
    "* Tensor network theory continues to be a vibrant and evolving field with ongoing research in various directions, such as the development of efficient tensor contraction algorithms, the application of tensor networks for understanding quantum phases of matter, the development of tensor network algorithms for quantum computing, and the application of tensor networks to machine learning.\n",
    "\n",
    "## Tensors\n",
    "\n",
    "Before discussing tensor networks, it is necessary to understand what tensors are.\n",
    "Furthermore, before really understanding tensors, it is instructive to reiterate some basic\n",
    "concepts of linear algebra for the case of vectors and matrices, which are nothing but\n",
    "specific cases of tensors. In fact, many of the concepts and ideas that are introduced and\n",
    "discussed are defined in terms of thinking of tensors as vectors or matrices.\n",
    "\n",
    "In what follows, vectors and matrices will be thought of from the viewpoint of computers,\n",
    "where they are represented using regular one- and two-dimensional arrays of either real or\n",
    "complex numbers. Nevertheless, much of the discussion can be trivially generalized to\n",
    "arbitrary vector spaces and linear maps.\n",
    "\n",
    "### Linear Algebra -- Vectors and Matrices\n",
    "\n",
    "In general, a vector is an object in a vector space, which can be described by a list of\n",
    "numbers that correspond to the components of the vector in some basis. For example, a vector\n",
    "in a two-dimensional space is in its most general form described by\n",
    "$$\\vec{v} = \\left[v_1, v_2\\right]^T$$\n",
    "\n",
    "As a reminder, the defining properties of vector spaces make sure that the following\n",
    "operations are well-defined:\n",
    "\n",
    "* Vectors can be added together, i.e. $\\vec{v} + \\vec{w}$ is a vector.\n",
    "* Vectors can be multiplied by scalars, i.e. $\\alpha \\vec{v}$ is a vector.\n",
    "* These operations behave as expected, i.e. there is a notion of associativity, commutativity, and distributivity.\n",
    "\n",
    "Given two such vector spaces (not necessarily distinct) it is possible to define a linear\n",
    "map between them, which is just a function that preserves the vector space structure. In\n",
    "other words, a linear map $A \\colon W ← V$ maps vectors from one vector space $V$ to another\n",
    "vector space $W$. Because of the structure of vector spaces, and the requirement of\n",
    "linearity, such a map is completely determined by its action on the basis vectors of $V$.\n",
    "This leads in a very natural way to the notion of a matrix by considering the following\n",
    "construction, where $v_i$ are the basis vectors of $V$ and $w_i$ are the basis vectors of $W$:\n",
    "\n",
    "```{math}\n",
    ":label: eq:linear_map\n",
    "\n",
    "A : W \\leftarrow V\n",
    "A : v ↦ w = A(v) = \\sum_j A_{ij} v_j\n",
    "```\n",
    "\n",
    "where $A_{ij}$ are the components of the matrix $A$ in the basis $v_i$ and $w_j$. In other\n",
    "words, the abstract notion of a linear map between vector spaces can be represented by a\n",
    "concrete matrix, and the action of the map is nothing but the usual matrix product.\n",
    "\n",
    "In particular, it is instructive to think of the columns of the matrix $A$ as labelling the\n",
    "components of the input vector space, while the rows label the component of the output\n",
    "vector space.\n",
    "\n",
    "### Multi-linear Algebra: Tensors and Tensor Products\n",
    "\n",
    "Using the same logic as above, it is possible to generalize the notion of a linear map by\n",
    "making use of the [tensor product](https://en.wikipedia.org/wiki/Tensor_product), which is\n",
    "nothing but an operation that can combine two vector spaces $V$ and $W$ into a new vector\n",
    "space $V \\otimes W$. The tensor product is defined in such a way that the combination of\n",
    "vectors from the original vector spaces preserves a natural notion of linearity, i.e. the\n",
    "following equality holds for all vectors $v \\in V$, $w \\in W$, and scalars $\\lambda$:\n",
    "\n",
    "```{math}\n",
    ":label: eq:tensor_product\n",
    "(\\lambda v) \\otimes w = v \\otimes (\\lambda w) = \\lambda (v \\otimes w)\n",
    "```\n",
    "\n",
    "This new vector space can be equipped with a canonical basis, which is constructed by taking\n",
    "the tensor product of the basis vectors of the original vector spaces. For example, if $V$\n",
    "and $W$ are two-dimensional vector spaces with basis vectors $v_i$ and $w_j$, respectively,\n",
    "then the basis vectors of $V \\otimes W$ are given by $v_i \\otimes w_j$. In other words, the\n",
    "vectors in $V \\otimes W$ are linear combinations of all combinatinos of the basis vectors of\n",
    "$V$ and $W$.\n",
    "\n",
    "When considering how to represent a vector in this new vector space, it can be written as a\n",
    "list of numbers that correspond to the components of the vector in that basis. For example,\n",
    "a vector in $V \\otimes W$ is described by:\n",
    "\n",
    "```{math}\n",
    ":label: eq:tensor_basis\n",
    "\n",
    "t = \\sum_{i_1,i_2} t_{i_1i_2} (v_{i_1} \\otimes w_{i_2})\n",
    "```\n",
    "\n",
    "Here, the tentative name $t$ was used to denote that this is in fact a tensor, where\n",
    "$t_{i_1i_2}$ are the components of that tensor $t$ in the basis $v_{i_1} \\otimes w_{i_2}$.\n",
    "Because of the induced structure of the tensor product, it is more natural and very common\n",
    "to express this object not just as a list of numbers, but by reshaping that list into a\n",
    "matrix. In this case, the components of the $i_1$-th row correspond to basis vectors that\n",
    "are built from $v_{i_1}$, and similarly the $i_2$-th column corresponds to basis vectors\n",
    "that are built from $w_{i_2}$.\n",
    "\n",
    "As the tensor product can be generalized to more than two vector spaces, this finally leads\n",
    "to the general definition of a tensor as an element of the vector space that is built up\n",
    "from the tensor product of an arbitrary number of vector spaces. Additionally, the\n",
    "components of these objects are then naturally laid out in a multi-dimensional array, which\n",
    "is then by a slight misuse of terminology also called a tensor.\n",
    "\n",
    "```{note}\n",
    "The reshaping operation of components from a list of numbers into a multi-dimensional array\n",
    "is nothing but a mapping between linear indices $I$ and Cartesian indices $i_1, i_2, \\cdots,\n",
    "i_N$. This is a very common and useful trick which allows reinterpreting tensors as vectors,\n",
    "or vice versa.\n",
    "```\n",
    "\n",
    "### Multi-linear Algebra: Tensors and Multi-linear Maps\n",
    "\n",
    "Due to the fact that the tensor product of vector spaces is a vector space in of itself, it\n",
    "is again possible to define linear maps between such vector spaces. Keeping in mind the\n",
    "definition of a linear map from {eq}`eq:linear_map`, the columns now label components of the\n",
    "input vector space, while the rows label components of the output vector space. Now however,\n",
    "the components of the input and output vector spaces are themselves comprised of a\n",
    "combination of basis vectors from the original vector spaces. If a linear order of these\n",
    "combinations can be established, the linear map can again be represented by a matrix:\n",
    "\n",
    "```{math}\n",
    ":label: eq:multilinear_map\n",
    "\\begin{array}{lcr}\n",
    "A \t& : & W_1 \\otimes W_2 \\otimes \\cdots \\otimes W_M \\leftarrow V_1 \\otimes V_2 \\otimes \\cdots \\otimes V_N \\\\\n",
    "\t& \t& v_1 \\otimes v_2 \\otimes \\cdots \\otimes v_N ↦ A(v) \\\\\n",
    "\t& \t&= w_1 \\otimes w_2 \\otimes \\cdots \\otimes w_M \\\\\n",
    "\t& \t&= \\sum_{j_1,j_2,\\cdots,j_N} A_{i_1,i_2,\\cdots,i_M;j_1,j_2,\\cdots,j_N} v_{1,j} \\otimes v_{2,j} \\otimes \\cdots \\otimes v_{N,j} \\\\\n",
    "\t& \t&= \\sum_{J} A_{I;J} v_J \\\\\n",
    "```\n",
    "\n",
    "The attentive reader might have already noted that the definition of a linear map as a\n",
    "matrix strongly resembles the definition of a vector in a tensor product vector space. This\n",
    "is not a coincidence, and in fact the two can easily be identified by considering the\n",
    "following identification (isomorphism):\n",
    "\n",
    "```{math}\n",
    ":label: eq:tensor_isomorphism\n",
    "V \\leftarrow W \\cong V \\otimes W^* \n",
    "```\n",
    "\n",
    "```{note}\n",
    "For finite-dimensional real or complex vector spaces without additional structure, this\n",
    "isomorphism is *trivial* and is nothing but the reshaping operation of the components of a\n",
    "vector into a matrix. However, note that this is a choice, which is not unique, and already\n",
    "differs for\n",
    "[row- and column-major order](https://en.wikipedia.org/wiki/Row-_and_column-major_order). In\n",
    "a more general setting, the identification between $V \\otimes W^*$ and $V \\leftarrow W$ is\n",
    "not an equivalence but an isomorphism. This means that it is still possible to relate one\n",
    "object to the other, but the operation is not necessarily trivial.\n",
    "```\n",
    "\n",
    "### Multi-linear Algebra: Conclusion\n",
    "\n",
    "The entire discussion can be summarized and leads to the following equivalent definitions of a tensor:\n",
    "\n",
    "* A tensor is an element of a tensor product of vector spaces, which can be represented as a multi-dimensional array of numbers that indicate the components along the constituent basis vectors. (a tensor is vector-like)\n",
    "* A tensor is a multi-linear map between vector spaces, which can be represented as a matrix that represents the action of the map on the basis vectors of the input vector space. (a tensor is matrix-like)\n",
    "\n",
    "The equivalence of these two definitions leads to the lifting of many important facets of linear algebra to the multi-linear setting."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Julia",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}